## Spark Streaming :o:


|          |                     |
| -------- | ------------------- |
| title    | Spark Streaming     | 
| status   | 10                  |
| section  | Streams             |
| keywords | Streams             |



Spark Streaming is a library built on top of Spark Core which enables
Spark to process real-time streaming data. The streaming jobs can be
written similar to batch jobs in Spark, using either Java, Scala or
Python. The input to Spark Streaming applications can be fed from
multiple data sources such HDFS, Kafka, Flume, Twitter, ZeroMQ, or
custom-defined sources. It also provides a basic abstraction called
Discretized Streams or DStreams to represent the continuous data
streams. Spark's API for manipulating these data streams is very
similar to the Spark Core's Resilient Distributed Dataset (RDD) API
which makes it easier for users to move between projects with stored
and real-time data as the learning curve is
short [@www-apache-spark-RDD].  Spark Streaming is designed to
provide fault-tolerance, throughput, and scalability. Examples of
streaming data are messages being published to a queue for real-time
flight status update or the log files for a production
server [@www-apache-spark-stream].


     
