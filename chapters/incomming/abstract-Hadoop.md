Apache Hadoop
-------------

The Apache HadoopÂ [@hid-sp18-515-www-hadoop] is an open-source software
designed for reliable, scalable, distributed computing. The Apache
Hadoop software library is a framework using simple programming models
that allows allows for the distributed processing of large data sets
across clusters of computers. It is designed to scale up from single
servers to thousands of machines, each offering local computation and
storage. The library is designed to detect and handle failures at the
application layer rather than rely on hardware to deliver
high-availability. Therefore, each of the computers in the cluster may
be prone to failures because it delivers a highly-available service on
top of a cluster of computers. The project includes these modules: 1.
Hadoop Common: The common utilities that support the other Hadoop
modules. 2. Hadoop Distributed File System (HDFS): A distributed file
system that provides high-throughput access to application data. 3.
Hadoop YARN: A framework for job scheduling and cluster resource
management. 4. Hadoop MapReduce: A YARN-based system for parallel
processing of large data sets.
